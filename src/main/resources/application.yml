server:
  port: 9091
  servlet:
    context-path: /
  compression:
    enabled: true
    mime-types: application/json,application/xml,text/html,text/xml,text/plain

spring:
  application:
    name: worksync-backend

  cloud:
    function:
      definition: registeredAppUsageConsumer;registeredAlertConsumer;registeredSecurityConsumer;vectorStore
    stream:
      kafka:
        binder:
          brokers: localhost:9092
          consumer-properties:
            spring.json.trusted.packages: "com.worksync.ai.model,com.worksync.ai.model.enums"
      default:
        consumer:
          configuration:
            key.deserializer: org.apache.kafka.common.serialization.StringDeserializer
            value.deserializer: org.springframework.kafka.support.serializer.JsonDeserializer
            spring.json.trusted.packages: "com.worksync.ai.model,com.worksync.ai.model.enums"
            spring.json.use.type.headers: false
            spring.json.value.default.type: "com.worksync.ai.model.AppUsageEvent"
      bindings:
        registeredAppUsageConsumer-in-0:
          destination: appUsage-topic
          group: work-sync-group
          content-type: application/json
          consumer:
            configuration:
              key.deserializer: org.apache.kafka.common.serialization.StringDeserializer
              value.deserializer: org.springframework.kafka.support.serializer.JsonDeserializer
              spring.json.trusted.packages: "com.worksync.ai.model,com.worksync.ai.model.enums"
              spring.json.value.default.type: "com.worksync.ai.model.AppUsageEvent"
              spring.json.use.type.headers: false
        registeredAlertConsumer-in-0:
          destination: alert-topic
          group: work-sync-group
        vectorStore-in-0:
          destination: vector-topic
          group: work-sync-group
        registeredSecurityConsumer-in-0:
          destination: security-topic
          group: work-sync-group

  ai:
    retry:
      max-attempts: 3
      initial-interval: 1000
      multiplier: 2.0
      max-interval: 10000
    openai:
      api-key: ${OPENROUTER_API_KEY:your-api-key-here}
      base-url: https://openrouter.ai/api/v1
      chat:
        model: mistralai/mistral-7b-instruct
        options:
          temperature: 0.7
          max-tokens: 1000
          http-headers:
            HTTP-Referer: "http://localhost:9091"
            X-Title: "WorkSync AI"
      embedding:
        model: mistralai/mistral-7b-instruct
        options:
          dimensions: 1536
          http-headers:
            HTTP-Referer: "http://localhost:9091"
            X-Title: "WorkSync AI"
    vectorstore:
      elasticsearch:
        initialize-schema: true
        index-name: worksync-summaries
        dimensions: 1536
        similarity: cosine

  elasticsearch:
    uris: http://localhost:9200
    connection-timeout: 5s
    socket-timeout: 10s
    username: ${ELASTICSEARCH_USERNAME:}
    password: ${ELASTICSEARCH_PASSWORD:}
    restclient:
      sniffer:
        interval: 10m
        delay-after-failure: 30s
    repositories:
      enabled: true

  task:
    scheduling:
      pool:
        size: 5
      thread-name-prefix: summary-scheduler-
    execution:
      pool:
        core-size: 8
        max-size: 16
        queue-capacity: 100
        keep-alive: 60s

  datasource:
    url: jdbc:postgresql://localhost:5432/worksync
    username: postgres
    password: postgres
    driver-class-name: org.postgresql.Driver
  jpa:
    hibernate:
      ddl-auto: update
    show-sql: true
    properties:
      hibernate:
        format_sql: true
        dialect: org.hibernate.dialect.PostgreSQLDialect
    database-platform: org.hibernate.dialect.PostgreSQLDialect

  data:
    elasticsearch:
      repositories:
        enabled: true
      cluster-name: docker-cluster
      cluster-nodes: localhost:9200

kafka:
  bootstrap-servers: ${KAFKA_BOOTSTRAP_SERVERS:localhost:9092}
  consumer:
    group-id: worksync-ai
    auto-offset-reset: earliest

priority-processing:
  critical: 0
  high: 100000
  normal: 200000

chatbot:
  rag:
    fallback:
      similarity-threshold: 0.2
      message: "I don't have enough data to answer that question. Please try asking something else."

# Optimized summarization configuration
summarization:
  # Use GPT-4 for better summarization quality
  model: "openai/gpt-4"
  # Lower temperature for more focused, consistent summaries
  temperature: 0.3
  # Limit tokens for concise summaries
  max-tokens: 800

summary:
  generation:
    interval: 3600
    enabled: true

management:
  endpoints:
    web:
      exposure:
        include: health,info,metrics,prometheus
  endpoint:
    health:
      show-details: always
  health:
    elasticsearch:
      enabled: true

logging:
  level:
    com.worksync.ai: DEBUG
    org.springframework: INFO
    org.elasticsearch: DEBUG
    org.springframework.ai: DEBUG
    org.springframework.kafka.support.serializer.JsonDeserializer: DEBUG
    org.hibernate.SQL: DEBUG
    org.hibernate.type.descriptor.sql.BasicBinder: TRACE
#    org.apache.kafka.clients.consumer: DEBUG
#    org.springframework.cloud.stream: DEBUG
#    org.springframework.integration: DEBUG
#    org.springframework.kafka: DEBUG